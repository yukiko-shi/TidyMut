# Data Saving Usage Guide

## Overview

This guide provides usage examples for saving cleaned datasets and cleaning artifacts, organized by database:

- [**HumanDomainome**](#human-domainome-database): Site-saturation mutagenesis of 500 human protein domains
    - [**Supplementary Table 2**](#supplementarytable2-cleaner): Fitness scores and errors.
    - [**Supplementary Table 4**](#supplementarytable4-cleaner): Homolog-averaged ∆∆G predictions across families mapped to homologous domains proteome-wide.
- [**ProteinGym**](#protein-gym-database): ProteinGym: Large-Scale Benchmarks for Protein Design and Fitness Prediction
- [**cDNAProteolysis**](#cdna-proteolysis-database): Mega-scale experimental analysis of protein folding stability in biology and design
- [**ddG-dTm Datasets**](#ddg-dtm-datasets): A collection of datasets providing single- and multiple-mutant measurements, labeled by thermodynamic parameters (ΔΔG, ΔTm)
- [**ArchStabMS1E10 Datasets**](#archstabms1e10-datasets): High-order multi-mutant libraries (“1e10”) measuring protein stability for GRB2-SH3 and SRC.

### Main idea

- **Main dataset** → use `dataset.save(path)`  
  → stored in the **tidymut format** (default `save_type="tidymut"`), which can be reloaded directly by tidymut.
- **Filtered / discarded / intermediate rows** (“artifacts”) → use `cleaning_pipeline.save_artifacts(path)`  
  → saved as a single **pickle** file (`.pkl`) containing multiple DataFrames.  
  → since pickle is binary and not convenient to view in VS Code, we **recommend reading it back in Python and exporting individual tables to CSV**.

## Prerequisites

You have already cleaned the raw data using the corresponding cleaning pipeline  
(see the *Data Cleaners Usage Guide*).

---

## Common Patterns

### Save the main cleaned dataset (tidymut format)

```python
# Default: save_type="tidymut"
dataset.save("path/to/output_dir")
```

### Save artifacts (filtered / discarded data) as pickle, then export to CSV

```python
# Save all artifacts (e.g., filtered-out rows, QC tables, per-step outputs)
cleaning_pipeline.save_artifacts("path/to/artifacts.pkl")

import pickle
import pandas as pd

# Load the binary pickle (not convenient to inspect directly in VS Code)
with open("path/to/artifacts.pkl", "rb") as f:
    artifacts = pickle.load(f)  # typically a dict: {name: DataFrame}

# Export each artifact DataFrame to a separate CSV for easy viewing
for key, df in artifacts.items():
    out_path = f"path/to/target_dir/{key}.csv"
    df.to_csv(out_path, index=False)
```

## Human Domainome Database

### SupplementaryTable2 Dataset

**save the main data**

```python
hd_dataset.save("path/to/output_dir")
```
**save the artifacts data**

```python
hd_cleaning_pipeline.save_artifacts("path/to/artifacts.pkl")

with open("path/to/artifacts.pkl", "rb") as f:
    obj = pickle.load(f)
    
for key, val in obj.items():
    out_path = f"path/to/target_dir/{key}.csv"
    val.to_csv(out_path, index=False)
```

### SupplementaryTable4 Dataset

**save the main data**

```python
hd_dataset.save("path/to/output_dir")
```
**save the artifacts data**

```python
hd_cleaning_pipeline.save_artifacts("path/to/artifacts.pkl")

with open("path/to/artifacts.pkl", "rb") as f:
    obj = pickle.load(f)
    
for key, val in obj.items():
    out_path = f"path/to/target_dir/{key}.csv"
    val.to_csv(out_path, index=False)
```

## cDNA Proteolysis Database

### ΔΔG Dataset

**save the main data**

```python
hd_dataset.save("path/to/output_dir")
```
**save the artifacts data**

```python
hd_cleaning_pipeline.save_artifacts("path/to/artifacts.pkl")

with open("path/to/artifacts.pkl", "rb") as f:
    obj = pickle.load(f)
    
for key, val in obj.items():
    out_path = f"path/to/target_dir/{key}.csv"
    val.to_csv(out_path, index=False)
```

### ΔG Dataset

**save the main data**

```python
hd_dataset.save("path/to/output_dir")
```
**save the artifacts data**

```python
hd_cleaning_pipeline.save_artifacts("path/to/artifacts.pkl")

with open("path/to/artifacts.pkl", "rb") as f:
    obj = pickle.load(f)
    
for key, val in obj.items():
    out_path = f"path/to/target_dir/{key}.csv"
    val.to_csv(out_path, index=False)
```

## ddG-dTm Datasets

**save the main data**

```python
ddgdtm_dataset.save("path/to/artifacts.pkl")
```
**save the artifacts data**

```python
ddgdtm_cleaning_pipeline.save_artifacts.save_artifacts("path/to/artifacts.pkl")

with open("path/to/artifacts.pkl", "rb") as f:
    obj = pickle.load(f)
    
for key, val in obj.items():
    out_path = f"path/to/target_dir/{key}.csv"
    val.to_csv(out_path, index=False)
```

## archstabms_1e10 Datasets

**save the main data**

```python
archstabms_dataset.save("path/to/output_dir")
```
**save the artifacts data**

```python
archstabms_cleaning_pipeline.save_artifacts("path/to/artifacts.pkl")

with open("path/to/artifacts.pkl", "rb") as f:
    obj = pickle.load(f)
    
for key, val in obj.items():
    out_path = f"path/to/target_dir/{key}.csv"
    val.to_csv(out_path, index=False)
```
